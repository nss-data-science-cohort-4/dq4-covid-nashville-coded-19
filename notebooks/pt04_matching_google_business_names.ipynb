{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Matching Violations to Google Business Address Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import time\n",
    "import operator\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import sparse_dot_topn.sparse_dot_topn as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view settings\n",
    "pd.set_option('display.max_rows', 800)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test code for reading in the json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Test to open a file\n",
    "# # with open('../data/google_places_results/results_01.json') as fi:\n",
    "# #     result = json.load(fi)\n",
    "\n",
    "# # # Alternate approach to reading in the json file\n",
    "# google_results = pd.read_json(r'../data/google_places_results/results_01.json')\n",
    "# google_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an empty dataframe with the columns we want, including all of the data from inside of, and outside of, the 'results' blob\n",
    "# column_names = ['mapped_location', 'address', 'name', 'types', 'address', 'lat', 'long']\n",
    "# result_addresses = pd.DataFrame(columns = column_names)\n",
    "\n",
    "# # Go through and open each json results file\n",
    "# for file in list(glob.glob('../data/google_places_results/*.json')):\n",
    "#     with open(file) as fi:\n",
    "#         result = json.load(fi)\n",
    "# # Write the contents of the 'results' field to a dataframe       \n",
    "#         google_results = pd.json_normalize(result)\n",
    "# # Clean up the dataframe columns\n",
    "#         google_results = google_results.drop(['results'], axis = 1)\n",
    "#         google_results.columns = ['mapped_location', 'address']\n",
    "# # Append the contents of each json file to the results dataframe\n",
    "#         result_addresses = result_addresses.append(google_results)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the json files in two parts\n",
    "\n",
    "One dataframe, `violations_addresses`, should only include the `mapped location` and `address` fields.  \n",
    "The second dataframe, `google_address`, should only include the fields we want from the `results` field.  \n",
    "To get only one business name per address, merge the `violations_addresses` and `google_address` fields on the `address` field and run `pd.drop_duplicates()` to get the first business name at the same address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe with the columns we want\n",
    "column_names = ['mapped_location', 'address']\n",
    "violations_addresses = pd.DataFrame(columns = column_names)\n",
    "\n",
    "# Go through and open each json results file\n",
    "for file in list(glob.glob('../data/google_places_results/*.json')):\n",
    "    with open(file) as fi:\n",
    "        result = json.load(fi)\n",
    "# Write the contents of the 'results' field to a dataframe       \n",
    "        google_results = pd.json_normalize(result)\n",
    "# Clean up the dataframe columns\n",
    "        google_results = google_results.drop(['results'], axis = 1)\n",
    "        google_results.columns = ['mapped_location', 'address']\n",
    "# Append the contents of each json file to the results dataframe\n",
    "        violations_addresses = violations_addresses.append(google_results)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations_addresses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see how many addresses were passed through to the Google API\n",
    "violations_addresses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many unique values there are\n",
    "print(violations_addresses.mapped_location.nunique())\n",
    "print(violations_addresses.address.nunique())\n",
    "# It looks like some mapped locations have different addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Reset the index so that there is a UID for each address/point - might be useful for deduping later\n",
    "# violations_addresses = violations_addresses.reset_index()\n",
    "# violations_addresses.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the column names\n",
    "violations_addresses.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe with the columns we want to get the fields we need out of the results blob\n",
    "column_names = ['name', 'types', 'address', 'lat', 'long']\n",
    "google_address = pd.DataFrame(columns = column_names)\n",
    "\n",
    "# Go through and open each json results file\n",
    "for file in list(glob.glob('../data/google_places_results/*.json')):\n",
    "    with open(file) as fi:\n",
    "        result = json.load(fi)\n",
    "# Write the contents of the 'results' field to a dataframe       \n",
    "        google_results = pd.json_normalize(result, 'results')\n",
    "# Clean up the dataframe columns\n",
    "        google_results = google_results.drop(['business_status', 'icon', 'place_id', 'rating', 'reference', 'scope', 'user_ratings_total', 'geometry.viewport.northeast.lat', 'geometry.viewport.northeast.lng', 'geometry.viewport.southwest.lat', 'geometry.viewport.southwest.lng', 'opening_hours.open_now', 'plus_code.compound_code', 'plus_code.global_code', 'photos', 'price_level', 'permanently_closed'], axis = 1)\n",
    "        google_results.columns = ['name', 'types', 'address', 'lat', 'long']\n",
    "# Append the contents of each json file to the results dataframe\n",
    "        google_address = google_address.append(google_results)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the dataframe we've created\n",
    "google_address.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations_addresses['address'] = violations_addresses['address'].str.replace(r',.+', '')\n",
    "violations_addresses['address'] = violations_addresses['address'].fillna('')\n",
    "violations_addresses['address'] = violations_addresses['address'].str.replace('nan', '')\n",
    "google_address['address'] = google_address['address'].str.replace(r',.+', '')\n",
    "google_address['address'] = google_address['address'].fillna('')\n",
    "google_address['address'] = google_address['address'].str.replace('nan', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "google_address['address'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_address['address'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_address.sort_values(by='address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_address = google_address.drop_duplicates(subset='address', keep='first')\n",
    "google_address.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_address[google_address['address'].isna() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Join the queried addresses to the Google API results. Keep all of the queried addresses\n",
    "# result_addresses_joined_01 = violations_addresses.merge(google_address, how = 'left', on = 'address')\n",
    "# result_addresses_joined_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the columns that were produced\n",
    "# result_addresses_joined_01.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Check how many business names were matched based on address\n",
    "# result_addresses_joined_01[result_addresses_joined_01['name'].isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_addresses_joined_01 = result_addresses_joined_01.drop_duplicates(subset='address', keep='first')\n",
    "# result_addresses_joined_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Explode the list in the 'types' column to get a row for each establishment type\n",
    "# result_addresses_joined_01 = result_addresses_joined_01.explode('types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Turn those rows into columns\n",
    "# result_addresses_joined_02 = pd.get_dummies(result_addresses_joined_01['types'], prefix = 'type').reset_index().groupby('index').sum()\n",
    "# result_addresses_joined_02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to do some data cleanup\n",
    "\n",
    "If we do a literal string match with light data cleaning (removing city info), we only get matches for 18% (861) of the original 4,698 addresses identified in the hubNashville 311 violations database.  \n",
    "\n",
    "Let's try [fuzzy string matching](https://medium.com/tim-black/fuzzy-string-matching-at-scale-41ae6ac452c2) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class for matching one list of strings to another\n",
    "class StringMatch():\n",
    "    \n",
    "    def __init__(self, source_names, target_names):\n",
    "        self.source_names = source_names\n",
    "        self.target_names = target_names\n",
    "        self.ct_vect      = None\n",
    "        self.tfidf_vect   = None\n",
    "        self.vocab        = None\n",
    "        self.sprse_mtx    = None\n",
    "        \n",
    "        \n",
    "    def tokenize(self, analyzer='char_wb', n=3):\n",
    "        '''\n",
    "        Tokenizes the list of strings, based on the selected analyzer\n",
    "        :param str analyzer: Type of analyzer ('char_wb', 'word'). Default is trigram\n",
    "        :param str n: If using n-gram analyzer, the gram length\n",
    "        '''\n",
    "        # Create initial count vectorizer & fit it on both lists to get vocab\n",
    "        self.ct_vect = CountVectorizer(analyzer=analyzer, ngram_range=(n, n))\n",
    "        self.vocab   = self.ct_vect.fit(self.source_names + self.target_names).vocabulary_\n",
    "        \n",
    "        # Create tf-idf vectorizer\n",
    "        self.tfidf_vect  = TfidfVectorizer(vocabulary=self.vocab, analyzer=analyzer, ngram_range=(n, n))\n",
    "        \n",
    "        \n",
    "    def match(self, ntop=1, lower_bound=0, output_fmt='df'):\n",
    "        '''\n",
    "        Main match function. Default settings return only the top candidate for every source string.\n",
    "        \n",
    "        :param int ntop: The number of top-n candidates that should be returned\n",
    "        :param float lower_bound: The lower-bound threshold for keeping a candidate, between 0-1.\n",
    "                                   Default set to 0, so consider all canidates\n",
    "        :param str output_fmt: The output format. Either dataframe ('df') or dict ('dict')\n",
    "        '''\n",
    "        self._awesome_cossim_top(ntop, lower_bound)\n",
    "        \n",
    "        if output_fmt == 'df':\n",
    "            match_output = self._make_matchdf()\n",
    "        elif output_fmt == 'dict':\n",
    "            match_output = self._make_matchdict()\n",
    "            \n",
    "        return match_output\n",
    "        \n",
    "        \n",
    "    def _awesome_cossim_top(self, ntop, lower_bound):\n",
    "        ''' https://gist.github.com/ymwdalex/5c363ddc1af447a9ff0b58ba14828fd6#file-awesome_sparse_dot_top-py '''\n",
    "        # To CSR Matrix, if needed\n",
    "        A = self.tfidf_vect.fit_transform(self.source_names).tocsr()\n",
    "        B = self.tfidf_vect.fit_transform(self.target_names).transpose().tocsr()\n",
    "        M, _ = A.shape\n",
    "        _, N = B.shape\n",
    "\n",
    "        idx_dtype = np.int32\n",
    "\n",
    "        nnz_max = M * ntop\n",
    "\n",
    "        indptr = np.zeros(M+1, dtype=idx_dtype)\n",
    "        indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "        data = np.zeros(nnz_max, dtype=A.dtype)\n",
    "\n",
    "        ct.sparse_dot_topn(\n",
    "            M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
    "            np.asarray(A.indices, dtype=idx_dtype),\n",
    "            A.data,\n",
    "            np.asarray(B.indptr, dtype=idx_dtype),\n",
    "            np.asarray(B.indices, dtype=idx_dtype),\n",
    "            B.data,\n",
    "            ntop,\n",
    "            lower_bound,\n",
    "            indptr, indices, data)\n",
    "\n",
    "        self.sprse_mtx = csr_matrix((data,indices,indptr), shape=(M,N))\n",
    "    \n",
    "    \n",
    "    def _make_matchdf(self):\n",
    "        ''' Build dataframe for result return '''\n",
    "        # CSR matrix -> COO matrix\n",
    "        cx = self.sprse_mtx.tocoo()\n",
    "\n",
    "        # COO matrix to list of tuples\n",
    "        match_list = []\n",
    "        for row,col,val in zip(cx.row, cx.col, cx.data):\n",
    "            match_list.append((row, self.source_names[row], col, self.target_names[col], val))\n",
    "\n",
    "        # List of tuples to dataframe\n",
    "        colnames = ['violations_addresses_index', 'violations_addresses_address', 'google_address_index', 'google_address_address', 'score']\n",
    "        match_df = pd.DataFrame(match_list, columns=colnames)\n",
    "\n",
    "        return match_df\n",
    "\n",
    "    \n",
    "    def _make_matchdict(self):\n",
    "        ''' Build dictionary for result return '''\n",
    "        # CSR matrix -> COO matrix\n",
    "        cx = self.sprse_mtx.tocoo()\n",
    "\n",
    "        # dict value should be tuple of values\n",
    "        match_dict = {}\n",
    "        for row,col,val in zip(cx.row, cx.col, cx.data):\n",
    "            if match_dict.get(row):\n",
    "                match_dict[row].append((col,val))\n",
    "            else:\n",
    "                match_dict[row] = [(col, val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, turn the unique values in the violations_addresses and results fields into lists\n",
    "violations_addresses_list = violations_addresses.address.unique().tolist()\n",
    "google_addresses_list = google_address.address.unique().tolist()\n",
    "# Then, apply the block of code above to create a new dataframe which shows the match of addresses\n",
    "addressmatch = StringMatch(violations_addresses_list, google_addresses_list)\n",
    "addressmatch.tokenize()\n",
    "violations_google_address_matches = addressmatch.match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check out the mapping of violations addresses and google API results\n",
    "violations_google_address_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations_google_address_matches.google_address_address.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations_google_address_matches.violations_addresses_address.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the Google API results that had multiple matches\n",
    "multi_match = violations_google_address_matches[violations_google_address_matches.duplicated(subset=['google_address_address']) == True].sort_values(by = 'google_address_index')\n",
    "multi_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe based on match score\n",
    "violations_google_address_matches = violations_google_address_matches.sort_values(by = ['google_address_address', 'score'], ascending = False)\n",
    "violations_google_address_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicates, only keep the highest-scoring match\n",
    "violations_google_address_matches = violations_google_address_matches.drop_duplicates(subset=['google_address_address'],keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations_google_address_matches.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations_google_address_matches.google_address_address.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Of the 4,697 addresses passed into the Google API, 3,922 were matched using this fuzzy string matching technique (83.5%). This is *much* better than our simple cleaning + string matching approach. It looks like there are some inexact matches and we could do more work to clean this up, but given the timeframe of this analysis, we feel okay with the results as they stand. Thanks for the suggestion, Tim!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge it all together\n",
    "\n",
    "Take the `violations_addresses` and `google_address` dataframes, merge them together based on the mapping dataframe `violations_google_address_matches`, and product a unique list of address-business matches in the `business_violations` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, merge on the violations_addresses dataframe\n",
    "business_violations = violations_addresses.merge(violations_google_address_matches, how = 'left', left_on = 'address', right_on = 'violations_addresses_address')\n",
    "business_violations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_violations.violations_addresses_address.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_violations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, merge on the google API results dataframe\n",
    "business_violations = business_violations.merge(google_address, how = 'left', left_on = 'google_address_address', right_on = 'address')\n",
    "business_violations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_violations.drop_duplicates(subset='google_address_address', keep='first')\n",
    "business_violations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_violations.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_violations = business_violations.drop(['address_x', 'violations_addresses_index', 'google_address_index', 'google_address_address', 'address_y'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "business_violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_violations = business_violations.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_violations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummyize the types to make it possible to group violations by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_violation_types = business_violations.explode('types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_violation_types.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For some reason, this drops 800+ entries so don't use it\n",
    "# business_violation_types = pd.concat([business_violation_types, pd.get_dummies(business_violation_types.types)], 1).groupby(['mapped_location', 'violations_addresses_address', 'score', 'name', 'lat', 'long']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_violation_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_violation_types = pd.get_dummies(business_violation_types['types'], prefix = 'type').reset_index().groupby('index').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "business_violation_types.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "business_violation_types.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the index into a column\n",
    "business_violation_types.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "business_violation_types.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_violation_types.index.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the full dataset with dummyized types\n",
    "business_violations_with_types = business_violations.merge(business_violation_types, how = 'outer', on = 'index')\n",
    "business_violations_with_types.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the business without any data?\n",
    "business_violations_with_types[business_violations_with_types['types'].isna() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_violations_with_types.columns = ['index', 'mapped_location', 'address', 'match_score', \n",
    "                                          'business_name', 'business_types', 'lat', 'long', 'type_accounting', 'type_amusement_park', \n",
    "                                          'type_aquarium', 'type_art_gallery', 'type_atm', 'type_bakery', 'type_bank', \n",
    "                                          'type_bar', 'type_beauty_salon', 'type_bicycle_store', 'type_book_store', \n",
    "                                          'type_bowling_alley', 'type_cafe', 'type_car_dealer', 'type_car_rental', \n",
    "                                          'type_car_repair', 'type_car_wash', 'type_cemetery', 'type_church', \n",
    "                                          'type_city_hall', 'type_clothing_store', 'type_convenience_store', \n",
    "                                          'type_courthouse', 'type_dentist', 'type_department_store', 'type_doctor', \n",
    "                                          'type_drugstore', 'type_electrician', 'type_electronics_store', \n",
    "                                          'type_establishment', 'type_finance', 'type_florist', 'type_food', \n",
    "                                          'type_funeral_home', 'type_furniture_store', 'type_gas_station', \n",
    "                                          'type_general_contractor', 'type_grocery_or_supermarket', 'type_gym', \n",
    "                                          'type_hair_care', 'type_hardware_store', 'type_health', 'type_home_goods_store', \n",
    "                                          'type_hospital', 'type_insurance_agency', 'type_jewelry_store', 'type_laundry', \n",
    "                                          'type_lawyer', 'type_library', 'type_liquor_store', 'type_local_government_office', \n",
    "                                          'type_locksmith', 'type_lodging', 'type_meal_delivery', 'type_meal_takeaway', \n",
    "                                          'type_movie_rental', 'type_movie_theater', 'type_moving_company', 'type_museum', \n",
    "                                          'type_night_club', 'type_painter', 'type_park', 'type_parking', 'type_pet_store', \n",
    "                                          'type_pharmacy', 'type_physiotherapist', 'type_place_of_worship', 'type_plumber', \n",
    "                                          'type_point_of_interest', 'type_police', 'type_post_office', 'type_premise', \n",
    "                                          'type_primary_school', 'type_real_estate_agency', 'type_restaurant', \n",
    "                                          'type_roofing_contractor', 'type_rv_park', 'type_school', 'type_secondary_school', \n",
    "                                          'type_shoe_store', 'type_shopping_mall', 'type_spa', 'type_stadium', 'type_storage', \n",
    "                                          'type_store', 'type_supermarket', 'type_synagogue', 'type_tourist_attraction', \n",
    "                                          'type_travel_agency', 'type_university', 'type_veterinary_care', 'type_zoo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_violations_with_types.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_violations_with_types.address.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_violations_with_types_for_merge = business_violations_with_types.drop(['index', 'mapped_location', 'match_score', 'lat', 'long'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the violations data and join it up to get business information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the COVID-19 violation reports dataset\n",
    "violations = pd.read_csv('../data/covid_violations.csv')\n",
    "violations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "violations_with_business_info = violations.merge(business_violations_with_types_for_merge, how = 'left', on = 'address')\n",
    "violations_with_business_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the violations results that had multiple matches\n",
    "multi_match_violations = violations_with_business_info[violations_with_business_info.duplicated(subset=['request_no']) == True].sort_values(by = 'request_no')\n",
    "multi_match_violations.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique request ids are getting duplicated?\n",
    "multi_match_violations.request_no.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the problem seems to be mostly with NaN values, do a quick dedupe\n",
    "violations_with_business_info = violations_with_business_info.drop_duplicates(subset=['request_no', 'address'],keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations_with_business_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations_with_business_info.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(violations_with_business_info.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write it out to a CSV\n",
    "violations_with_business_info.to_csv('../data/violations_with_business_info.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
